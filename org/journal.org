* February

** 2016-02-02 Notes from Arnaud
- [[http://mescal.imag.fr/membres/arnaud.legrand/research/M2R_boast.pdf][internships]]
- Stephan Wild's slides at JLPC
- Pointer from Emmanuel Aggulo: [[https://www.gerad.ca/~orban/papers.html][Dominique Orban]]
  - http://dpo.github.io/opal/ One of these articles mentions the
    ability to handle nicely discrete and continuous factors.
  - http://www.gerad.ca/~orban/_static/templating.pdf
- Travaux de Grigori Fursin sur http://www.ctuning.org/ (machine
  learning/big data).
- [[https://hal.inria.fr/hal-00872482][Old article mentioning early results with BOAST]]

Have a look at
https://en.wikipedia.org/wiki/Genetic_algorithm#Limitations
** 2016-02-02
*** Summary discussion with Arnaud
    Optimization can be seen as:
    - _Minimization problem_
      E.g gradient approach if the function is convexe and second
      derivative \to good conditions.
    - _Fixed Point problem_
      If k-Lipschitz property (f is contraction mapping) can converge very quickly.
      
    Those methods can be generic, we can apply f multiple time to have
    good "conditions".
