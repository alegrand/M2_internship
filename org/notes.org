#+AUTHOR:      Steven QUINITO MASNADA
#+BABEL: :tangle yes

* Installation
  Here the installation that download and install al the necessary tools.
** Base
   #+begin_src sh :results output :exports both :tangle ../setup.sh
     #!/bin/bash
   #+end_src
*** Directory
    #+begin_src sh :results output :exports both :tangle ../setup.sh
      ROOT_DIR=$PWD
      BOAST_DIR=$ROOT_DIR/boast
      DARWIN_DIR=$ROOT_DIR/darwinning
      BOAST_LIG_DIR=$ROOT_DIR/boast-lig
      TOOLS_DIR=$ROOT_DIR/apps
    #+end_src
*** Ruby
   #+begin_src sh :results output :exports both :tangle ../setup.sh
     echo "=========== Install ruby ============="
     gksudo "apt-get install -y ruby ruby-dev"
   #+end_src
*** Install fortran
    #+begin_src sh :results output :exports both :tangle ../setup.sh
      echo "=========== Install fortran ============="
      gksudo "apt-get install -y gfortran"
    #+end_src
*** Install Opencl
   #+begin_src sh :results output :exports both :tangle ../setup.sh
     echo "=========== Install OpenCL ============="
     gksudo "apt-get install -y ocl-icd-libopencl1 ocl-icd-opencl-dev"
     if [ "$HOSTNAME" = "titan" ] ; then
         mkdir -p $TOOLS_DIR
         cd $TOOLS_DIR
         gksudo "apt-get install -y rpm alien libnuma1"
         wget http://registrationcenter.intel.com/irc_nas/4181/opencl_runtime_14.2_x64_4.5.0.8.tgz
         tar xvzf opencl_runtime_14.2_x64_4.5.0.8.tgz
         cd pset_opencl_runtime_14.1_x64_4.5.0.8/rpm/
         fakeroot alien --to-deb opencl-1.2-intel-cpu-4.5.0.8-1.x86_64.rpm
         gksudo "dpkg -i opencl-1.2-intel-cpu_4.5.0.8-2_amd64.deb"
         gksudo "mkdir -p /etc/OpenCL"
         gksudo "mkdir -p /etc/OpenCL/vendors"
         gksudo "cp /opt/intel/opencl-1.2-4.5.0.8/etc/intel64.icd /etc/OpenCL/vendors/"
     else
         gksudo "apt-get install -y nvidia-opencl-icd-340"
     fi
   #+end_src

** Boast
   #+begin_src sh :results output :exports both :tangle ../setup.sh
     echo "========= Installing Boast ========="
     cd $ROOT_DIR
     git clone git@github.com:Nanosim-LIG/boast.git
     cd $BOAST_DIR
     gem build *.gemspec
     gem install --user-install *.gem
   #+end_src
   
   To avoid retriving from internet:
   #+begin_src sh :results output :exports both
     gem install --user-install -l *.gem
   #+end_src

** Darwining
    #+begin_src sh :results output :exports both :tangle ../setup.sh
      echo "========= Installing Darwining ========="
      cd $ROOT_DIR
      git clone git@github.com:Nanosim-LIG/darwinning.git
      cd $DARWIN_DIR
      gem build *.gemspec
      gem install --user-install *.gem
    #+end_src

   Install minitest:
    #+begin_src sh :results output :exports both :tangle ../setup.sh
      gem install --user-install minitest
    #+end_src

** Boast-lig
    #+begin_src sh :results output :exports both :tangle ../setup.sh
      echo "========= Installing Boast-lig ========="
      cd $ROOT_DIR
      git clone https://forge.imag.fr/anonscm/git/boast/boast.git boast-lig
    #+end_src
   
* Topic
** Overview
   To optimize correctly an application, we have to find what are the
   best combinations of optimization parameters and compilation 
   flags. This can be modelized as a mathematical optimization problem 
   where we want to minize or maximize y = f(x_0, x_1, ..., x_n) with x
   being an optimization parameter or a compilation flag. The problem
   is the search space being huge and we cannot use the brute
   force approach and test every combinations. Thus we need to explore
   only parts of the search space to try to find an optimal
   solution. For this genetic algorithms works reasonnably well, they
   test very different possibilities keep the best ones and make try 
   combinations of them, keep the bests and continue until they find
   an optimal one.
** Problem   
   - The first thing is that we do not really why genetic algorithm
     perform well.
   
   - The second is genetic algorithms are a random approach and do not
     use any knwoledge about the problem. Knowing that it might be
     possible to do things smarter and it might exist methods that use
     knwoledge about the problem and that are more efficient than
     a random approach.

* Goal
  - Understand why certain methods work?
  - Try to characterize the problem of optimization.
    If we know the problem's shape we can use this knowledge to be
    more efficient.
  - Find efficient solution and implement it as an optimizer.
* State of the art
** Empirical approach
*** Global search
**** Random
**** Genetic algorithms
**** Simulated Annealing
*** Local search
**** Nelder Mead Simplex
**** Surogates-based search
* Biblio
** A comparison of Search Heuristics for Empirical Code Optimization
*** Summary
    - Comparing:
      - Random
      - Simplex
      - Particule Swarm
      - Orthogonal
      - Genetic Algorithm
      - Simulated Annealing
    - Combining code optimization and compilation flags
*** Link
    http://netlib.org/utk/people/JackDongarra/PAPERS/gco_search.pdf
*** Bibtex
    #+BEGIN_SRC 
    @conference {icl:418,
	title = {A Comparison of Search Heuristics for Empirical Code Optimization},
	booktitle = {The 3rd international Workshop on Automatic Performance Tuning},
	year = {2008},
	note = {5},
	pages = {421-429},
	publisher = {IEEE},
	organization = {IEEE},
	address = {Tsukuba, Japan},
	author = {Keith Seymour and Haihang You and Jack Dongarra}
    }
    #+END_SRC
** Can Search algorithms save large-scale automatic performance tuning?
*** Summary
    - *Formulation* of the autotuning search problem *as mathematical*
      *optimization problem*.
    - *Algorithms* need to be *adapted to the autotuning problem* \to na√Øve
      Nelder Mead simplex vs modified one gives better results. 
      Due to the fact the normal version is made for continuous
      variables and here they only tested discrete variables.
    - Random search seems to be efficient for problems when the
      problem has lots of parameters that give good results. It also
      tends to have a bigger rate of failure (compilation or runtime
      errors) because does not keep track of hidden incorrect
      combination of parameters. 
*** Questions
    - What is the importance of the formalization in a mathematical
      optimization problem? I didn't really get how they use this
      particularity. 
*** Link
    http://ac.els-cdn.com/S1877050911002924/1-s2.0-S1877050911002924-main.pdf?_tid=4f7211d8-c9b7-11e5-ab07-00000aacb35d&acdnat=1454422665_1e1560e8379ea8cb8f740e08b18b05bf
*** Bibtex
    #+BEGIN_SRC 
    @article{Balaprakash20112136,
        title = "Can search algorithms save large-scale automatic performance tuning?",
        author = "Prasanna Balaprakash and Stefan M. Wild and Paul D. Hovland",
        journal = "Procedia Computer Science",
        volume = "4",
        pages = "2136 - 2145",
        year = "2011",
        note = "Proceedings of the International Conference on Computational Science, ICCS 2011",
        issn = "1877-0509",
        doi = "10.1016/j.procs.2011.04.234"
    }
    #+END_SRC
** An Experimental Study of Global and Local Search Algoritms in Empirical Perfomance Tuning
*** Summary
    - Study the comparison between global and local search
      - Random
      - Genetic Algorithm
      - Simulated annealing
      - Nelder Mead simplex
      - Surrogate based search
    - Strong time constraint \to getting the best variant in a short
      time
    - Local algo are generally better
      - Nelder Mead
      - Surrogates based search
      - But initial parameters have to be chosen carefully \to sensitive
    - Global are generally longer 
      - due to their explorative nature
      - reducing their degree of exploration improve their results
*** Questions / remarks
    - Maybe their experiments have some biais because at they end find
      that reducing the degree of exploration improve the performances
      of the global algorithms. So it is possible that they did not
      adapted correctly the GA and SA to the autotuning search
      problem. And maybe they compare them with optimized version of
      NM and SBS.
*** Link
    http://www.mcs.anl.gov/papers/P1995-0112.pdf
*** Bibtex
    #+BEGIN_SRC 
    @incollection{PBSWPHLNCS13,
    title       = "An Experimental Study of Global and Local Search Algorithms in Empirical Performance Tuning",
    author      = "Prasanna Balaprakash and Stefan M. Wild and Paul D. Hovland",
    booktitle   = "High Performance Computing for Computational Science - VECPAR 2012, 
    10th International Conference, Kobe, Japan, July 17-20, 2012, Revised Selected Papers.",
    series      = "Lecture Notes in Computer Science",  
    editors     = "M.J. Dayd\'e, O. Marques, K. Nakajima",    
    year        = "2013",
    publisher   = "Springer",
    pages       = "pp. 261--269",
    doi         = "10.1007/978-3-642-38718-0_26",
    isbn        = "978-3-642-38717-3"
    }
    #+END_SRC
** Deconstructing Iterative optimization
*** Summary
    - Compiler flags optimizations only
    - It is possible to learn a combination of optimization from data
      set that suit most of other data set \to analyzing the datasets.
    - Interesting to see how they study :
      - If the iterative optimization is efficient across datasets 
        - They collected a big sample, 
        - Found what are the best optimizations 
        - Kept common optimizations 
        - apply it to others samples
      - Why it is efficient \to by analyzing the results.
*** Link
    http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.308.5061&rep=rep1&type=pdf
*** Remarks
    - Interesting to see how I can get into the problem, what kind of
      question I could ask to myself, and for the methodology.
** Templating and Automatic Code for Performance with Python
*** Summary
    - They don't use a empirical or heuristic method but they use a
      mesh-adaptive direct search
    - Tested 3 variant of NOMAD but no comparison with empirical approach
*** Link
    http://www.gerad.ca/~orban/_static/templating.pdf
*** Bibtex
    #+BEGIN_SRC 
    @book{orban2011templating,
    title={Templating and Automatic Code Generation for Performance with Python},
    author={Orban, D. and Groupe d'{\'e}tudes et de recherche en analyse des d{\'e}cisions (Montr{\'e}al, Qu{\'e}bec)},
    series={Cahiers du G{\'E}RAD},
    url={https://books.google.fr/books?id=QfwutwAACAAJ},
    year={2011},
    publisher={Groupe d'{\'e}tudes et de recherche en analyse des d{\'e}cisions}
    }
    #+END_SRC
** Mesh Adaptive Direct Search Algorithms for Constrained Optimization
*** Summary
*** Link
    http://epubs.siam.org/doi/pdf/10.1137/040603371
*** Bibtex
    #+BEGIN_SRC 
    @ARTICLE{Audet04meshadaptive,
    author = {Charles Audet and J. E},
    title = {Mesh adaptive direct search algorithms for constrained optimization},
    journal = {SIAM Journal on optimization},
    year = {2004},
    volume = {17},
    pages = {2006}
    }
    #+END_SRC
** Crowdtuning: Systematizing auto-tuning using predictive modeling and crowdsourcing
*** Summary
    - Combine machine learning and statistical analysis
      The idea is that some problems have similarities and require the
      similar optimizations. Assuming that we can use previous
      knowldege (model) to find what could be the best configuration
      to explore first.
    - Learn correlation between code optimization, compilation flags,
      hardware, and programs
    - Collecting big sample to modelize and predict behavior
*** Link
    https://hal.inria.fr/hal-00944513/document
*** Bibtex
    #+BEGIN_SRC 
    @inproceedings{memon:hal-00944513,
    TITLE = {{Crowdtuning: systematizing auto-tuning using predictive modeling and crowdsourcing}},
    AUTHOR = {Memon, Abdul Wahid and Fursin, Grigori},
    URL = {https://hal.inria.fr/hal-00944513},
    BOOKTITLE = {{PARCO mini-symposium on ''Application Autotuning for HPC (Architectures)''}},
    ADDRESS = {Munich, Germany},
    YEAR = {2013},
    MONTH = Sep,
    PDF = {https://hal.inria.fr/hal-00944513/file/paper.pdf},
    HAL_ID = {hal-00944513},
    HAL_VERSION = {v1},
    }
    #+END_SRC
* Question
  - Autotuning is an optimization problem but what are its specifities
    if it had some?
