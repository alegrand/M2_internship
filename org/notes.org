#+TITLE:       Notes
#+AUTHOR:      Steven QUINITO MASNADA
#+BABEL:       :tangle yes

* Topic
** Overview
   To optimize correctly an application, we have to find what are the
   best combinations of optimization parameters and compilation 
   flags. This can be modelized as a mathematical optimization problem 
   where we want to minize or maximize y = f(x_0, x_1, ..., x_n) with x
   being an optimization parameter or a compilation flag. The problem
   is the search space being huge and we cannot use the brute
   force approach and test every combinations. Thus we need to explore
   only parts of the search space to try to find an optimal
   solution. For this genetic algorithms works reasonnably well, they
   test very different possibilities keep the best ones and make try 
   combinations of them, keep the bests and continue until they find
   an optimal one.
** Problem   
   - The first thing is that we do not really why genetic algorithm
     perform well.
   
   - The second is genetic algorithms are a random approach and do not
     use any knwoledge about the problem. Knowing that it might be
     possible to do things smarter and it might exist methods that use
     knwoledge about the problem and that are more efficient than
     a random approach.

* Goal
  - Try to characterize the problem of optimization.
    If we know the shape of the problem we can use this knowledge to be
    more efficient.
  - Understand why certain methods works better for some class of problem.
  - Find efficient solution and implement it as an optimizer.
* State of the art
** Search methods 
*** Random
*** Genetic algorithms
*** Simulated Annealing
     - Made for discrete search space.
     - Explore the space using neighbours (states that are similars to
       the considered state).
     - The algo visit neighbours to find better values.
     - Can accept worse neighbours
     - Temperature \to time
     - Energie \to result return by the system
     - Start Global to slowly go to local.
     - The temperature defined the probability of accepting worse
       value \to avoid to get stuck in a bad local optima. As long as
       the temperature decrease the probability of accepting worse
       value decrease and make smaller moves. 
       
*** Nelder Mead Simplex
     - Direct search methods
     - Derivative-free
*** Surogates-based search
     - Use approximation models about what could be the behavior of an
       application in order to make an estimation of the correct
       parameters. The effectiveness relies on the accuracy of the
       model. The idea is to use knowledge about the problem to know
       where to concentrate the search. 
     - The difficult part here is the construction of the model
       # How to build a model?
     - Some methodology using this approach:
       - _Response surface methodology_
         - Relationship between response variables and explanatory
           variables through a set of experiments
         - Model = second degree polynomial
       - _Kriging_ 
       - _Support Vector Machine_
         - Classification \to gives in which category belong the problem
           and thus to which model suit the most.
       - _Trust-region algorithm_
         Try to fit a quadratic model into a "trusted region". If it
         fits the region is expanded(increases the size) else
         contracted(decreases the size).
* Biblio
** A comparison of Search Heuristics for Empirical Code Optimization
*** Summary
    - Comparing:
      - Random \to simple and effective
      - Simplex
      - Particule Swarm
      - Orthogonal
      - Genetic Algorithm
      - Simulated Annealing
    - Combining code optimization and compilation flags
*** Link
    http://netlib.org/utk/people/JackDongarra/PAPERS/gco_search.pdf
*** Bibtex
    #+BEGIN_SRC 
    @conference {icl:418,
	title = {A Comparison of Search Heuristics for Empirical Code Optimization},
	booktitle = {The 3rd international Workshop on Automatic Performance Tuning},
	year = {2008},
	note = {5},
	pages = {421-429},
	publisher = {IEEE},
	organization = {IEEE},
	address = {Tsukuba, Japan},
	author = {Keith Seymour and Haihang You and Jack Dongarra}
    }
    #+END_SRC
** Can Search algorithms save large-scale automatic performance tuning?
*** Summary
    - *Formulation* of the autotuning search problem *as mathematical*
      *optimization problem*.
    - *Algorithms* need to be *adapted to the autotuning problem* \to na√Øve
      Nelder Mead simplex vs modified one gives better results. 
      Due to the fact the normal version is made for continuous
      variables and here they only tested discrete variables.
    - Random search seems to be efficient for problems when the
      problem has lots of parameters that give good results. It also
      tends to have a bigger rate of failure (compilation or runtime
      errors) because does not keep track of hidden incorrect
      combination of parameters. 
*** Questions
**** What is the importance of the formalization in a mathematical optimization problem? 
     # I didn't really get how they use this particularity. 
     It helps to modelize the problem as a function and to correctly
     find what to take into account. For example what is the most
     suited metric for the objective function \to for optimization
     problem it is more logical to take a metric we know the bound.
     E.g. time to compute a pixel (bounded by zero) vs Flops (unknown
     bound) 
*** Link
    http://ac.els-cdn.com/S1877050911002924/1-s2.0-S1877050911002924-main.pdf?_tid=4f7211d8-c9b7-11e5-ab07-00000aacb35d&acdnat=1454422665_1e1560e8379ea8cb8f740e08b18b05bf
*** Bibtex
    #+BEGIN_SRC 
    @article{Balaprakash20112136,
        title = "Can search algorithms save large-scale automatic performance tuning?",
        author = "Prasanna Balaprakash and Stefan M. Wild and Paul D. Hovland",
        journal = "Procedia Computer Science",
        volume = "4",
        pages = "2136 - 2145",
        year = "2011",
        note = "Proceedings of the International Conference on Computational Science, ICCS 2011",
        issn = "1877-0509",
        doi = "10.1016/j.procs.2011.04.234"
    }
    #+END_SRC
** An Experimental Study of Global and Local Search Algoritms in Empirical Perfomance Tuning
*** Summary
    - Study the comparison between global and local search
      - Random
      - Genetic Algorithm
      - Simulated annealing
      - Nelder Mead simplex
      - Surrogate based search \to trust-region algorithm
    - Strong time constraint \to getting the best variant in a short
      time
    - Local algo
      - Nelder Mead
      - Surrogates based search
      - Very efficient if we know where to search
        Initial parameters have to be chosen carefully \to sensitive
    - Global algo
      - Generally longer due to their explorative nature
      - Reducing their degree of exploration improve their results
        But again here we need to know where to search
*** Questions / remarks
    - Average on only 10 run maybe not enough \to missing confidence interval
    - We cannot really link the different experiment they did because
      each time they benchmark a different application.
    - We already know that they use a version of the simplex adapted
      to the autotuning problem but how well adapted are the other
      algorithms? Especially GA and SA. In the last experiment
      reducing the exploration degree henances their
      performances. Does that mean that for the previous experiences
      GA and SA are not well adapted and there is some biais?
      Generally we lack information about how are tuned GA and SA 
      so we cannot really make some conclusion.
    - If we restrict to much the factor of exploration of GA and Sa
      are they equivalent to local search?
*** Link
    http://www.mcs.anl.gov/papers/P1995-0112.pdf
*** Bibtex
    #+BEGIN_SRC 
    @incollection{PBSWPHLNCS13,
    title       = "An Experimental Study of Global and Local Search Algorithms in Empirical Performance Tuning",
    author      = "Prasanna Balaprakash and Stefan M. Wild and Paul D. Hovland",
    booktitle   = "High Performance Computing for Computational Science - VECPAR 2012, 
    10th International Conference, Kobe, Japan, July 17-20, 2012, Revised Selected Papers.",
    series      = "Lecture Notes in Computer Science",  
    editors     = "M.J. Dayd\'e, O. Marques, K. Nakajima",    
    year        = "2013",
    publisher   = "Springer",
    pages       = "pp. 261--269",
    doi         = "10.1007/978-3-642-38718-0_26",
    isbn        = "978-3-642-38717-3"
    }
    #+END_SRC
** Deconstructing Iterative optimization
*** Summary
    - Compiler flags optimizations only
    - It is possible to learn a combination of optimization from data
      set that suit most of other data set \to analyzing the datasets.
    - Interesting to see how they study :
      - If the iterative optimization is efficient across datasets 
        - They collected a big sample, 
        - Found what are the best optimizations 
        - Kept common optimizations 
        - apply it to others samples
      - Why it is efficient \to by analyzing the results.
*** Link
    http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.308.5061&rep=rep1&type=pdf
*** Remarks
    - Interesting to see how I can get into the problem, what kind of
      question I could ask to myself, and for the methodology.
** Direct search
*** Templating and Automatic Code for Performance with Python
**** Summary
    - They don't use a empirical or heuristic method but they use a
      mesh-adaptive direct search
    - Tested 3 variant of NOMAD but no comparison with empirical
      approach.       
**** Questions/Remarks
    - No comparison with other state of the art approaches (empirical
      methods)
    - This paper does not seems to to directly bring
      interesting stuffs, it is more presentation of another code
      generator. But there are interesting link of mesh-adaptive
      direct search.
**** Link
    http://www.gerad.ca/~orban/_static/templating.pdf
**** Bibtex
    #+BEGIN_SRC 
    @book{orban2011templating,
    title={Templating and Automatic Code Generation for Performance with Python},
    author={Orban, D. and Groupe d'{\'e}tudes et de recherche en analyse des d{\'e}cisions (Montr{\'e}al, Qu{\'e}bec)},
    series={Cahiers du G{\'E}RAD},
    url={https://books.google.fr/books?id=QfwutwAACAAJ},
    year={2011},
    publisher={Groupe d'{\'e}tudes et de recherche en analyse des d{\'e}cisions}
    }
    #+END_SRC
*** "Direct Search" Solution of Numerical and Statistical Problems
**** Summary
     - Two type of moves:
       - Exploratory \to get knowledge(inference about successful or not
         moves). Sampling the space. Try to find better solution
         elsewhere. Moves in directions defined by patterns of a size
         that evolves (increases if sucessful move else decreases).
         Define an area for the current iterate.
       - Pattern \to Use knowledge to minimize f. Search around a base
         point.
**** Questions/Remarks
     - I don't really get how a pattern is built.
**** Link
    http://delivery.acm.org/10.1145/330000/321069/p212-hooke.pdf?ip=194.199.27.221&id=321069&acc=ACTIVE%20SERVICE&key=7EBF6E77E86B478F%2E9B0CC472860F67C6%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=751594268&CFTOKEN=50107111&__acm__=1455030106_ccd8cb9aa66e698e79840f74cfa6aa91
**** Bibtex
    #+BEGIN_SRC 
    @article{Hooke:1961:DSS:321062.321069,
    author = {Hooke, Robert and Jeeves, T. A.},
    title = {`` Direct Search'' Solution of Numerical and Statistical Problems},
    journal = {J. ACM},
    issue_date = {April 1961},
    volume = {8},
    number = {2},
    month = apr,
    year = {1961},
    issn = {0004-5411},
    pages = {212--229},
    numpages = {18},
    url = {http://doi.acm.org/10.1145/321062.321069},
    doi = {10.1145/321062.321069},
    acmid = {321069},
    publisher = {ACM},
    address = {New York, NY, USA},
    } 
    #+END_SRC
*** On The Convergence of Pattern Search Algorithm
**** Summary
**** Link
     http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.4715&rep=rep1&type=pdf
**** Bibtex
*** Generalized pattern searches with derivative information
**** Summary
    - Direct search / pattern search methods
    - For unconstrained and linearly constrained problems
    - Two phases:
      - SEARCH \to global exploration to find interesting regions
        - Mesh construction :
          - possible to use any technics \to Genetic algo, surrogate
            based searches, etc...
          - Try to improve the current optimal
          - Exploratory move not restricted by the size of the mesh \to
            more various exploration at the begining.
      - POLL \to local exploration to examine interesting regions
        (around a base). Points to visit are define by a pattern
        
    - Henanced version of pattern search \to Use derivative information
      when available to speed the POLL phase
**** Questions/remarks
**** Links
    http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=35C815204C5D2F2BD69ADA2BD527763A?doi=10.1.1.381.255&rep=rep1&type=pdf
**** Bibtex
    #+BEGIN_SRC 
    @article{DBLP:journals/mp/AbramsonAD04,
    author    = {Mark A. Abramson and
                 Charles Audet and
                 John E. Dennis Jr.},
    title     = {Generalized pattern searches with derivative information},
    journal   = {Math. Program.},
    volume    = {100},
    number    = {1},
    pages     = {3--25},
    year      = {2004},
    url       = {http://dx.doi.org/10.1007/s10107-003-0484-5},
    doi       = {10.1007/s10107-003-0484-5},
    timestamp = {Wed, 06 May 2015 19:49:45 +0200},
    biburl    = {http://dblp.uni-trier.de/rec/bib/journals/mp/AbramsonAD04},
    bibsource = {dblp computer science bibliography, http://dblp.org}
    }
    #+END_SRC
*** Mesh Adaptive Direct Search Algorithms for Constrained Optimization
**** Summary
     - Extended version of Generalized Pattern Search
     - Infinte set of directions
     - Works with nonsmooth functions
**** Questions/remarks
**** Link
    http://epubs.siam.org/doi/pdf/10.1137/040603371
**** Bibtex
    #+BEGIN_SRC 
    @ARTICLE{Audet04meshadaptive,
    author = {Charles Audet and J. E},
    title = {Mesh adaptive direct search algorithms for constrained optimization},
    journal = {SIAM Journal on optimization},
    year = {2004},
    volume = {17},
    pages = {2006}
    }
    #+END_SRC
** Crowdtuning: Systematizing auto-tuning using predictive modeling and crowdsourcing
*** Summary
    - Combine machine learning and statistical analysis
      The idea is that some problems have similarities and require
      similar optimizations. Assuming that we can use previous 
      knowldege (model) to find what could be the best configurations
      to explore first.
    - Learn correlation between code optimization, compilation flags,
      hardware, and programs
    - Collecting big sample to modelize and predict behavior
*** Link
    https://hal.inria.fr/hal-00944513/document
*** Bibtex
    #+BEGIN_SRC 
    @inproceedings{memon:hal-00944513,
    TITLE = {{Crowdtuning: systematizing auto-tuning using predictive modeling and crowdsourcing}},
    AUTHOR = {Memon, Abdul Wahid and Fursin, Grigori},
    URL = {https://hal.inria.fr/hal-00944513},
    BOOKTITLE = {{PARCO mini-symposium on ''Application Autotuning for HPC (Architectures)''}},
    ADDRESS = {Munich, Germany},
    YEAR = {2013},
    MONTH = Sep,
    PDF = {https://hal.inria.fr/hal-00944513/file/paper.pdf},
    HAL_ID = {hal-00944513},
    HAL_VERSION = {v1},
    }
    #+END_SRC
** Milepost GCC: machine learning enabled self-tuning
*** Summary
    - Learning over iterative optimization
    - Can target multi-objective optimization
    - Used two models:
      - Probabilistic:
        - Attributes are independants
        - Use probability distribution of good solutions and take the
          mode. 
        - To learn the model:
          - They first learn the distribution of good solution (one
            that bring a speedup more than 98%) on each programs (in
            the training set)using uniform sampling. 
          - They estimate the distribution of a program by using the
            euclidian distance (take the closest program \to 1 nearst
            neighbors) 
      - Transductive:
        - Analyze interaction between attributes
        - Model built using a decision tree
        - Correlation of compilation optimization and program
          characteristics that give good speedup
        - Easier to analyze than probabilistic model
    - Using the solutions of the closest neighbors works for the same plateform
*** Questions/remarks
**** For the probabilistic model, do they use a different uniform sampling on each program? 
**** Why in the probabilistic model good solutions are those that give 98% of speedup  and in the transductive model it is 95% ?
**** Using the solutions of the closest neighbors works for the same plateform, does this work in cross platform context?
*** Links
    http://fursin.net/papers/fkmp2011.pdf
*** Bibtex
    #+BEGIN_SRC 
    @article{fursin:hal-00685276,
    TITLE = {{Milepost GCC: Machine Learning Enabled Self-tuning Compiler}},
    AUTHOR = {Fursin, Grigori and Kashnikov, Yuriy and Memon, Abdul Wahid and Chamski, Zbigniew and Temam, Olivier and Namolaru, Mircea and Yom-Tov, Elad and Mendelson, Bilha and Zaks, Ayal and Courtois, Eric and Bodin, Fran{\c c}ois and Barnard, Phil and Ashton, Elton and Bonilla, Edwin and Thomson, John and Williams, Christopher K. I. and O'Boyle, Michael},
    URL = {https://hal.inria.fr/hal-00685276},
    JOURNAL = {{International Journal of Parallel Programming}},
    PUBLISHER = {{Springer Verlag}},
    VOLUME = {39},
    PAGES = {296-327},
    YEAR = {2011},
    DOI = {10.1007/s10766-010-0161-2},
    HAL_ID = {hal-00685276},
    HAL_VERSION = {v1},
    }
    #+END_SRC
** Exploiting Performance Portability in Search Algorithms for Autotuning
*** Summary
    - Correlation between peformance and *code optimization across*
      *platforms*
    - Supervised machine learning to build the model \to recursive
      partitionning approach \to random forest
    - Two strategy:
      - Pruning \to *Bad* optimizations an machine A are expected to be
        *bad* on machine B.
        Use the model from machine A, sample at random machine B,
        make prediction and evaluate only set that are *smaller* than a
        thresold. 
      - Biasing \to *Good* optimizations an machine A are expected to be
        *good* on machine B
        Almost like pruning but try unevaluated configurations with the
        *smallest* predicted value.
    - The more the architectures similar are the more correlated the
      results. But what is the more important is the correlation
      between high performing combination.
    - They found correlation in high peforming combination between
      intel CPU(Sandy Bridge and Westmere), IBM Power7 and Xeon Phi
      but not really with ARM. 
*** Questions / remarks
*** Link
    http://www.mcs.anl.gov/papers/P5397-0915.pdf
*** Bibtex
    #+BEGIN_SRC 
    @techreport{RoyBalHovWil2015,
    author = {A. Roy and P. Balaprakash and P. D. Hovland and S. M. Wild},
    date-added = {2015-09-11 18:59:31 +0000},
    date-modified = {2015-09-22 03:02:04 +0000},
    institution = {Argonne National Laboratory},
    number = {ANL/MCS-P5400-0915},
    title = {Exploiting performance portability in search algorithms for autotuning},
    year = {2015}
    }
    #+END_SRC
* Questions
** Autotuning is an optimization problem but what are its specificities if it had some? 
   Because if there are some specificities, we can then modelize and
   approximite what are the best parameters. 
** What is the size of the search space of BOAST?
    There is no given size, it depends on the user's problem
